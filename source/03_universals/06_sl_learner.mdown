**Prerequisites**

- posets (lattices)

# The lattice space of $n$-gram grammars

The general take-home message of this unit has been that a surprising number of seemingly unrelated language universals can be stated as monotonicity properties over ordered sets.
This includes both substantive universals ($^*$ABA, PCC) and formal universals (adjunct island constraint).
But monotonicity is even more general.
We conclude our journey with a brief look at how language acquisition might be informed by monotonicity, too.

## SL and learning

Recall that a (negative) strictly $n$-local (SL-$n$) grammar is a finite set of $n$-grams such that a string is well-formed iff it does not contain any of the $n$-grams as a substring (after adding $n-1$ instances of \$ on each side of the string).

\begin{exercise}
As a reminder for yourself, write down an SL grammar for an attested linguistic phenomenon.
\end{exercise}

Suppose we have a phenomenon that is SL.
Then of course we can write an SL grammar for it.
But how exactly do we come up with the grammar?
And more generally, how can this phenomenon be acquired?
Is there some algorithm that a child might use to identify the correct grammar for an SL process?

One answer might be: move on, nobody cares.
But learning is in fact the most important question in linguistics.
Every linguistic phenomenon must be learnable from limited input data, for the simple reason that this is exactly what millions of children do on a daily basis.
And if you consider yourself to be solely interested in computational applications, the problem actually gets more pressing.
Computational linguists rarely write grammars by hand.
This happens in some very small and specialized domains, but in general grammars are learned automatically from a data sample.
Grammatical inference of this kind requires a robust learning algorithm.

For SL, the learning algorithm is very simple, and that's thanks to monotonicity.

## The lattice of grammars

Suppose that we pick a fixed value for $n$ and write down all positive $n$-gram grammars.
We could also use all negative grammars, but using positive grammars makes things more intuitive later on.
Either way we have a finite set of $n$-gram grammars, which we call $\mathcal{G}$.
Since each $n$-gram grammar is itself a set, we can order the set of $n$-gram grammars by the subset relation.
We refer to this resulting structure as $\mathbb{G} \is \tuple{\mathcal{G}, \subseteq}$.

\begin{example}
Suppose $n = 2$ and our alphabet $\Sigma$ contains only $a$.
Once we add the edge marker, there are 16 distinct bigram grammars over $\Sigma_\$$.

<ol>
<li>$\emptyset$</li>
<li>$\setof{\$\$}$</li>
<li>$\setof{\$a}$</li>
<li>$\setof{a\$}$</li>
<li>$\setof{aa}$</li>
<li>$\setof{\$\$, \$a}$</li>
<li>$\setof{\$\$, a\$}$</li>
<li>$\setof{\$\$, aa}$</li>
<li>$\setof{\$a, a\$}$</li>
<li>$\setof{\$a, aa}$</li>
<li>$\setof{a\$, aa}$</li>
<li>$\setof{\$\$, \$a, a\$}$</li>
<li>$\setof{\$\$, \$a, aa}$</li>
<li>$\setof{\$\$, a\$, aa}$</li>
<li>$\setof{\$a, a\$, aa}$</li>
<li>$\setof{\$\$, \$a, a\$, aa}$</li>
</ol>

Ordering these sets by the subset relation $\subseteq$ yield the following structure.

\input_mid{./lattice_bigrams.tikz}
\end{example}

The structure in the above example is rather special.
For any two grammars $G_1$ and $G_2$, there is a unique smallest grammar $G_\vee$ such that $G_\vee \subseteq G_1, G_2$.
Similarly, there's also a unique largest grammar $G_\wedge$ such that $G_1, G_2 \subseteq G_\wedge$.
In other words, $\mathcal{G}$ is both a meet semilattice and a join semilattice.
This makes $\mathbb{G}$ a **lattice**.

In fact, $\mathbb{G}$ is a **powerset lattice**.
The powerset of a set $S$ is $\wp(S) \is \setof{ X \mid X \subseteq S}$.
In other words, the powerset of $S$ consists of all subsets of $S$, including the empty set and $S$ itself.
The grammars we listed above are all subsets of $\Sigma_\$^2$.
Therefore, $\mathbb{G}$ is $\wp(\Sigma_\$^2)$, sorted by $\subseteq$.

That's all nice and dandy, let's see why this is more than just a mathematical curiosity.

## The lattice of SL languages

Each grammar $G$ of $\mathcal{G}$ generates a unique string language $L(G)$.
This language is just the set of strings that are well-formed with respect to $G$.
Let us call the set of all these languages $\mathcal{L}$.
More formally, $\mathcal{L} \is \setof{ L(G) \mid G \in \mathcal{G} }$.
Just like $\mathcal{G}$, we can order $\mathcal{L}$ by $\subseteq$ to obtain a lattice $\mathbb{L} \is \tuple{\mathcal{L}, \subseteq}$.

\begin{example}
Recall that all grammars in $\mathcal{G}$ are assumed to be positive.
Then the languages generated by these grammars are as follows:

<ol>
<li>$L(\emptyset) = \emptyset$</li>
<li>$L(\setof{\$\$}) = \setof{\emptystring}$</li>
<li>$L(\setof{\$a}) = \emptyset$</li>
<li>$L(\setof{a\$}) = \emptyset$</li>
<li>$L(\setof{aa}) = \emptyset$</li>
<li>$L(\setof{\$\$, \$a}) = \setof{\emptystring}$</li>
<li>$L(\setof{\$\$, a\$}) = \setof{\emptystring}$</li>
<li>$L(\setof{\$\$, aa}) = \setof{\emptystring}$</li>
<li>$L(\setof{\$a, a\$}) = \setof{a}$</li>
<li>$L(\setof{\$a, aa}) = \emptyset$</li>
<li>$L(\setof{a\$, aa}) = \emptyset$</li>
<li>$L(\setof{\$\$, \$a, a\$}) = \setof{\emptystring, a}$</li>
<li>$L(\setof{\$\$, \$a, aa}) = \setof{\emptystring}$</li>
<li>$L(\setof{\$\$, a\$, aa}) = \setof{\emptystring}$</li>
<li>$L(\setof{\$a, a\$, aa}) = \setof{a, aa, aaa, \ldots} = a^+$</li>
<li>$L(\setof{\$\$, \$a, a\$, aa}) = \setof{\emptystring, a, aa, aaa, \ldots} = a^*$</li>
</ol>
\end{example}

Let us put the lattices from the two examples next to each other.
We also add frames around nodes to indicate which grammar generates which language.

\input_large{./lattice_bigrams_match.tikz}

Notice anything special?
Whenever two grammars stand in the subset relation, their languages do, too.
That is to say, $G \subseteq G'$ implies $L(G) \subseteq L(G')$.
Once again we've stumbled across monotonicity.
The relation between grammars and their languages is order-preserving: move to a larger positive grammar, and you either get the same string language or a larger one.
The monotonicity of this relation allows for a very natural learning algorithm for SL languages.

\begin{exercise}
Suppose that all members of $\mathcal{G}$ are negative grammars.
Draw the corresponding lattice of languages, and connect each grammar to its language with a colored line.
\end{exercise}

\begin{exercise}
What is the relation between negative grammars and their languages?
Is it still monotonic?
If so, is it the same kind of monotonicity?
\end{exercise}

## Learning SL languages

Suppose that we have to learn some SL language $L$ from a sample of well-formed strings.
All we know about $L$ is that it is, say, SL-3, and what its alphabet $\Sigma$ looks like.
In order to learn $L$, all we have to do is the following:

1. Construct $\mathbb{G} \is \setof{\wp(\Sigma_\$^n), \subseteq}$, where $n$ is the maximum value required for $L$ (the value of $n$ is given by assumption).
1. Set the conjecture target grammar $G$ to the empty grammar, which is the lowest point of $\mathbb{G}$.
1. Look at a string $s$ in the data sample.
   If $s \notin L(G)$, find the smallest $G' \subsetneq G$ that contains every $n$-gram in $s$.
   This is the new target grammar.
1. Continue doing this until $G$ no longer changes.

Given sufficient data, this simple procedure will always yield a grammar $G$ that generates $L$.
The grammar does not undergenerate, nor does it overgenerate.
Overgeneration is ruled out by monotonicity: by picking the smallest $G' \supseteq G$ that contains all $n$-grams of $s$, we also pick the smallest possible extension of $L(G)$ that contains $s$.

Note also that this learning algorithm is very fast.
Every change of the target grammar moves the learner upward by at least one level in the lattice.
As each level contains multiple grammars, each update step rules out many grammars at once.
So even though the space of SL-$n$ grammars can be very large, a few steps suffice to home in on the grammar that generates the desired language.

\begin{example}
The lattice in the previous examples contains 16 elements, but only has 5 distinct levels.
Since we already start at level 1, it takes at most 4 updates to find the matching grammar.
\end{example}

\begin{exercise}
Formulate an analogous learning algorithm that instead operates with the lattice of negative SL-$n$ grammars. 
\end{exercise}

## Universals in learning

Albeit simple, the learning algorithm is quite smart.
It builds on the realization that relations don't just hold of linguistic entities, they even extend to entire languages.
Learning becomes much easier once one realizes that the space of possible languages isn't flat but contains hidden structure, too.

However, in order for this to work as desired, the learning algorithm needs to know two parameters in advance: the alphabet, and the value for $n$.
But this is not too much of a problem in practice as both the alphabet and the value for $n$ can be approximated.
For natural language, the alphabet can contain all possible sounds, even if some do not actually occur in the target language.
And since $n$-grams can always be padded out to some higher value, we can also set $n$ to some safe upper bound like $10$.
It is fairly unlikely that any natural language is SL-$n$ only for some $n > 10$.
The downside of picking a large alphabet and high $n$ is that the lattice becomes much bigger, which reduces the speed of the learning algorithm.

\begin{example}
Suppose the target language only contains 40 out of 100 possible sounds.
It is also SL-4 rather than SL-10.
With 100 sounds there are $2^{100^{10}} = 2^{1$e$+20}$ distinct SL-10 grammars.
This is a huge number: in relation to this number, the amount of atoms in the universe is like a grain of sand to Mount Everest.

With 40 sounds and 4-grams, there are "only" $2^{40^4} = 2^{2,560,000}$ distinct grammars.
That's still a mindboggingly large number, but remember that each update rules out a good chunk of those grammars.
So the learning algorithm can still converge fairly quickly on the correct grammar.
\end{example}

Children presumably start out with some safe value for the alphabet $\Sigma$ and $n$ as a genetically encoded universal about the maximum complexity of languages.
As they figure out more about the target language, they prune down $\Sigma$ and $n$ to reduce the size of grammar lattice, which speeds up the speed of acquisition.
